# ğŸ® Deep Reinforcement Learning with Neural Networks

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.0+-red.svg)](https://pytorch.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![License](https://img.shields.io/badge/License-Educational-green.svg)](#)

## ğŸ“‹ Overview

This project explores **Deep Reinforcement Learning** through the implementation of **Deep Q-Networks (DQN)** and **Prioritized Experience Replay (PER)**. The assignment demonstrates value function approximation algorithms across multiple environments, showcasing the power of neural networks in solving complex sequential decision-making problems.

### ğŸ¯ Key Objectives
- Implement Deep Q-learning based on DeepMind's groundbreaking paper
- Explore various neural network architectures for different observation types
- Compare standard DQN with Prioritized Experience Replay improvements
- Evaluate performance across multiple OpenAI Gym environments

---

## ğŸ—ï¸ Project Structure

```
ğŸ“¦ Deep-RL-Project
â”œâ”€â”€ ğŸ““ roopeshv_hvenkatr_assignment2_part1.ipynb    # Neural Network Architectures
â”œâ”€â”€ ğŸ““ roopeshv_hvenkatr_assignment2_part2.ipynb    # DQN Implementation & Training
â”œâ”€â”€ ğŸ¯ roopeshv_hvenkatr_assignment2_part2_dqn_*.pickle    # Trained DQN Models
â”œâ”€â”€ ğŸ¯ roopeshv_hvenkatr_assignment2_part3_per_*.pickle    # Trained PER Models
â”œâ”€â”€ ğŸ“„ README.md                                    # Project Documentation
â””â”€â”€ ğŸ“„ LICENSE                                      # License Information
```

---

## ğŸ§  Part 1: Neural Network Architectures

### ğŸŒ Wumpus World Environment

The first part implements various neural network architectures for the **Wumpus World** environment - a classic AI problem from Russell & Norvig's textbook.

#### Environment Details:
- **Grid Size**: 6Ã—6 (36 total blocks)
- **Objective**: Collect gold while avoiding the Wumpus and pits
- **Sensors**: Breeze (near pits) and Stench (near Wumpus)

#### ğŸ”§ Implemented Network Architectures:

| Observation Type | Input Dimension | Architecture | Output |
|------------------|----------------|--------------|---------|
| **Integer** | 1 | Dense(1â†’64â†’4) | Q-values |
| **Vector (2D)** | 2 | Dense(2â†’64â†’4) | Q-values |
| **Vector (One-hot)** | 36 | Dense(36â†’64â†’4) | Q-values |
| **Image** | 84Ã—84 | Conv2D(128)â†’Dense(64â†’4) | Q-values |
| **Float** | 1 | Dense(1â†’64â†’4) | Q-values |
| **Multi-Discrete** | 36 | Dense(36â†’64â†’4)â†’Sigmoid | Action probabilities |
| **Continuous** | 36 | Dense(36â†’64â†’1)â†’Tanh | Continuous action |

---

## ğŸš€ Part 2: Deep Q-Network Implementation

### ğŸ® Environments Tested

#### 1. ğŸ GridWorld (Custom Mario Environment)
- **Theme**: Super Mario-inspired grid navigation
- **Goal**: Reach Princess Peach while avoiding enemies
- **Challenges**: Negative rewards for hitting obstacles
- **State Space**: 6Ã—6 grid positions
- **Action Space**: 4 discrete actions (Up, Down, Left, Right)

#### 2. ğŸ”ï¸ MountainCar-v0
- **Objective**: Drive an underpowered car up a steep hill
- **Challenge**: Car must build momentum by oscillating
- **State Space**: Position and velocity (continuous)
- **Action Space**: 3 discrete actions (Left, Nothing, Right)

#### 3. ğŸ¯ CartPole-v1
- **Objective**: Balance a pole on a moving cart
- **Challenge**: Prevent pole from falling over
- **State Space**: Cart position, velocity, pole angle, pole velocity
- **Action Space**: 2 discrete actions (Left, Right)

### ğŸ§® DQN Algorithm Features

#### Core Components:
- **Experience Replay Buffer**: Stores transitions for stable learning
- **Target Network**: Separate network for stable Q-value targets
- **Îµ-Greedy Exploration**: Balances exploration vs exploitation
- **Soft Target Updates**: Gradual target network updates (Ï„ = 0.005)

#### Network Architecture:
```python
class Net(nn.Module):
    def __init__(self, n_observations, n_actions):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(n_observations, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, n_actions)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

#### Hyperparameters:
- **Learning Rate**: 1e-3
- **Batch Size**: 64
- **Gamma (Discount Factor)**: 0.99
- **Epsilon Decay**: 0.009
- **Buffer Size**: 10,000
- **Target Update Frequency**: Soft updates every step

---

## â­ Part 3: Prioritized Experience Replay (PER)

### ğŸ¯ Enhancement Overview

PER improves upon standard DQN by:
- **Prioritizing Important Transitions**: Samples experiences based on TD-error magnitude
- **Importance Sampling**: Corrects bias introduced by non-uniform sampling
- **Faster Learning**: Focuses on transitions where the agent can learn the most

### ğŸ“Š Performance Comparison

| Environment | Standard DQN | DQN + PER | Improvement |
|-------------|--------------|-----------|-------------|
| **GridWorld** | Baseline | Enhanced convergence | â¬†ï¸ Faster learning |
| **MountainCar** | Baseline | Better sample efficiency | â¬†ï¸ Reduced episodes |
| **CartPole** | Baseline | More stable training | â¬†ï¸ Consistent performance |

---

## ğŸ“ˆ Results & Analysis

### ğŸ† Training Performance

The implementation demonstrates successful learning across all environments:

1. **GridWorld**: Agent learns optimal path to Princess Peach
2. **MountainCar**: Solves the momentum-building challenge
3. **CartPole**: Achieves stable pole balancing

### ğŸ“Š Key Metrics Tracked:
- **Episode Rewards**: Total reward per episode
- **Epsilon Decay**: Exploration rate over time
- **Loss Values**: Training stability indicators
- **Success Rate**: Task completion percentage

### ğŸ” Observations:
- PER shows improved sample efficiency in sparse reward environments
- Convolutional networks effectively process visual observations
- Proper hyperparameter tuning crucial for stable learning

---

## ğŸ› ï¸ Installation & Usage

### Prerequisites:
```bash
pip install torch torchvision
pip install gymnasium
pip install matplotlib numpy
pip install jupyter notebook
```

### Running the Code:
1. **Part 1 - Neural Architectures**:
   ```bash
   jupyter notebook roopeshv_hvenkatr_assignment2_part1.ipynb
   ```

2. **Part 2 - DQN Training**:
   ```bash
   jupyter notebook roopeshv_hvenkatr_assignment2_part2.ipynb
   ```

3. **Load Pre-trained Models**:
   ```python
   import torch
   model = torch.load('roopeshv_hvenkatr_assignment2_part2_dqn_cartpole.pickle')
   ```

---

## ğŸ”¬ Technical Implementation Details

### ğŸ§ª Experience Replay Buffer
```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque([], maxlen=capacity)
    
    def push(self, state, action, reward, next_state):
        self.buffer.append((state, action, reward, next_state))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
```

### ğŸ¯ Prioritized Experience Replay
- **Priority Calculation**: |TD-error| + Îµ
- **Sampling Probability**: P(i) = p_i^Î± / Î£ p_k^Î±
- **Importance Sampling Weight**: w_i = (1/N * 1/P(i))^Î²

### ğŸ”„ Training Loop
1. **Environment Interaction**: Agent takes action, observes reward
2. **Experience Storage**: Store transition in replay buffer
3. **Batch Sampling**: Sample mini-batch from buffer
4. **Q-Learning Update**: Update policy network
5. **Target Update**: Soft update of target network

---

## ğŸ“š Key Concepts Demonstrated

### ğŸ“ Reinforcement Learning Fundamentals:
- **Markov Decision Processes (MDPs)**
- **Value Function Approximation**
- **Temporal Difference Learning**
- **Exploration vs Exploitation Trade-off**

### ğŸ¤– Deep Learning Integration:
- **Function Approximation with Neural Networks**
- **Convolutional Networks for Visual Input**
- **Experience Replay for Stable Learning**
- **Target Networks for Training Stability**

### ğŸ”§ Advanced Techniques:
- **Prioritized Experience Replay**
- **Double DQN (implicit through target networks)**
- **Epsilon-Greedy with Exponential Decay**
- **Gradient Clipping for Training Stability**

---

## ğŸ¯ Future Enhancements

### Potential Improvements:
- **Double DQN**: Reduce overestimation bias
- **Dueling DQN**: Separate value and advantage estimation
- **Rainbow DQN**: Combine multiple improvements
- **Distributional RL**: Model full return distribution

### Additional Environments:
- **Atari Games**: Visual complexity
- **Continuous Control**: DDPG/TD3 algorithms
- **Multi-Agent**: Competitive/Cooperative scenarios

---

## ğŸ“– References

1. **Mnih, V. et al.** (2015). Human-level control through deep reinforcement learning. *Nature*
2. **Schaul, T. et al.** (2016). Prioritized Experience Replay. *ICLR*
3. **Russell, S. & Norvig, P.** Artificial Intelligence: A Modern Approach
4. **Sutton, R. & Barto, A.** Reinforcement Learning: An Introduction

---

## ğŸ« Academic Context

**This project is for educational purposes as part of CSE 446/546 coursework.**

### Course Learning Outcomes:
- âœ… Understanding of value function approximation
- âœ… Implementation of deep reinforcement learning algorithms
- âœ… Analysis of different neural network architectures
- âœ… Evaluation of algorithm improvements and variations
- âœ… Practical experience with OpenAI Gym environments

---

## ğŸ‘¥ Contributors

**Authors**: Roopesh V, Hvenkatr  
**Course**: CSE 446/546 - Reinforcement Learning  
**Institution**: University of Washington  

---

## ğŸ“„ License

This project is licensed under the Educational Use License - see the [LICENSE](LICENSE) file for details.

---

*Made with â¤ï¸ for advancing the understanding of Deep Reinforcement Learning*